{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahxA2CxNPWK1"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Force upgrade all critical libraries to match Qwen 2.5 requirements\n",
        "pip install -U -q git+https://github.com/huggingface/transformers\n",
        "pip install -U -q accelerate bitsandbytes qwen-vl-utils\n",
        "pip install -q pillow requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import requests\n",
        "import re\n",
        "import os\n",
        "from PIL import Image, ImageDraw\n",
        "from io import BytesIO\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "# 1. CONFIGURATION & ROBUST LOADING\n",
        "\n",
        "MODEL_ID = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "\n",
        "def load_agent():\n",
        "    print(f\"Loading {MODEL_ID} in 4-bit mode...\")\n",
        "    print(\"(This configuration prevents OOM on Colab's T4 GPU)\")\n",
        "\n",
        "    # 1. Configure 4-bit Quantization (Fixes Memory Issues)\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "\n",
        "    # 2. Load Model\n",
        "    try:\n",
        "        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "            MODEL_ID,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ LOAD ERROR: {e}\")\n",
        "        print(\"Did you restart the session after installing libraries?\")\n",
        "        raise e\n",
        "\n",
        "    # 3. Load Processor\n",
        "    processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "    print(\"âœ… Agent Loaded and Ready!\")\n",
        "    return model, processor\n",
        "\n",
        "\n",
        "# 2. INFERENCE LOGIC (With Safety Resize)\n",
        "\n",
        "def run_inference(model, processor, image, user_prompt):\n",
        "\n",
        "    # --- SAFETY RESIZE (Fixes 'CUDA Out of Memory') ---\n",
        "    # Qwen 'native resolution' explodes memory on T4 if image > 2000px.\n",
        "    # We cap it at 1024px.\n",
        "    max_dimension = 1024\n",
        "    if max(image.size) > max_dimension:\n",
        "        image.thumbnail((max_dimension, max_dimension))\n",
        "        print(f\"âš ï¸ Image resized to {image.size} for GPU safety.\")\n",
        "\n",
        "    # Create message structure\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image},\n",
        "                {\"type\": \"text\", \"text\": user_prompt},\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Process inputs\n",
        "    text_prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    image_inputs, video_inputs = process_vision_info(messages)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=[text_prompt],\n",
        "        images=image_inputs,\n",
        "        videos=video_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    inputs = inputs.to(model.device)\n",
        "\n",
        "    # Generate\n",
        "    print(\"ðŸ¤” Agent is looking and thinking...\")\n",
        "    generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
        "\n",
        "    # Decode result\n",
        "    generated_ids_trimmed = [\n",
        "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    output_text = processor.batch_decode(\n",
        "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "    )[0]\n",
        "\n",
        "    return output_text, image\n",
        "\n",
        "\n",
        "# 3. VISUALIZATION (Bounding Box Drawer)\n",
        "\n",
        "def visualize_results(image, output_text):\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    width, height = image.size\n",
        "\n",
        "    print(f\"\\n--- Agent Response ---\\n{output_text}\\n----------------------\")\n",
        "\n",
        "    # Regex to capture [x1, y1, x2, y2]\n",
        "    # Qwen uses 0-1000 coordinate system\n",
        "    boxes = re.findall(r'\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]', output_text)\n",
        "\n",
        "    if boxes:\n",
        "        print(f\"Found {len(boxes)} object(s). Drawing boxes...\")\n",
        "        for box in boxes:\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "\n",
        "            # Convert 0-1000 scale to real pixels\n",
        "            abs_x1 = (x1 / 1000) * width\n",
        "            abs_y1 = (y1 / 1000) * height\n",
        "            abs_x2 = (x2 / 1000) * width\n",
        "            abs_y2 = (y2 / 1000) * height\n",
        "\n",
        "            # Draw Red Box\n",
        "            draw.rectangle([abs_x1, abs_y1, abs_x2, abs_y2], outline=\"red\", width=4)\n",
        "\n",
        "        display(image)\n",
        "    else:\n",
        "        print(\"No bounding boxes returned. (If you wanted detection, ask: 'Detect the...')\")\n",
        "        display(image)\n",
        "\n",
        "\n",
        "# 4. MAIN INTERFACE\n",
        "\n",
        "def main():\n",
        "    # Check for T4 GPU\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"âš ï¸ CRITICAL WARNING: No GPU detected.\")\n",
        "        print(\"Go to Runtime > Change runtime type > Select T4 GPU.\")\n",
        "        return\n",
        "\n",
        "    model, processor = load_agent()\n",
        "\n",
        "    while True:\n",
        "        print(\"\\n\" + \"=\"*40)\n",
        "        print(\"ðŸ“· QWEN2.5 MULTIMODAL AGENT\")\n",
        "        print(\"1. Use Demo Image (URL)\")\n",
        "        print(\"2. Upload Your Own Image\")\n",
        "        print(\"q. Quit\")\n",
        "        choice = input(\"Select: \")\n",
        "\n",
        "        if choice.lower() == 'q':\n",
        "            break\n",
        "\n",
        "        image = None\n",
        "        # Option 1: URL\n",
        "        if choice == '1':\n",
        "            url = input(\"Enter Image URL (or press Enter for default): \").strip()\n",
        "            if not url:\n",
        "                url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n",
        "            try:\n",
        "                response = requests.get(url, stream=True)\n",
        "                image = Image.open(BytesIO(response.content))\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Option 2: Upload\n",
        "        elif choice == '2':\n",
        "            uploaded = files.upload()\n",
        "            if not uploaded: continue\n",
        "            filename = list(uploaded.keys())[0]\n",
        "            image = Image.open(filename)\n",
        "\n",
        "        if image:\n",
        "            # Show input (resized for display)\n",
        "            display_img = image.copy()\n",
        "            display_img.thumbnail((300, 300))\n",
        "            print(\"\\nInput:\")\n",
        "            display(display_img)\n",
        "\n",
        "            print(\"\\nMODES:\")\n",
        "            print(\"1. Chat / Description\")\n",
        "            print(\"2. Object Detection (Grounding)\")\n",
        "            mode = input(\"Select Mode (1 or 2): \")\n",
        "\n",
        "            if mode == '2':\n",
        "                target = input(\"What to find? (e.g. 'the dog', 'the car'): \")\n",
        "                prompt = f\"Detect {target}. Return the bounding boxes.\"\n",
        "            else:\n",
        "                prompt = input(\"Ask a question: \")\n",
        "\n",
        "            # Run (Pass COPY so we don't resize the original if loop continues)\n",
        "            response_text, processed_img = run_inference(model, processor, image.copy(), prompt)\n",
        "\n",
        "            # Visualize\n",
        "            visualize_results(processed_img, response_text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "n9lSKaedPaUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SnX6YTjSPYAh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}